# ğŸš€ Federated Learning with FLWR - Optimized Client
### Author - Aryan Pandey
## ğŸ“Œ Overview
This project implements a **customized federated learning client** using [Flower (FLWR)](https://flower.ai/) with **client-side optimizations** to make training efficient on **constrained, embedded, and edge devices** (â‰¤2GB RAM).

### ğŸ”¹ **Key Optimizations**
âœ… **Model Optimization** â†’ Quantization & Pruning for memory-efficient training.  
âœ… **Partial Local Training** â†’ Train only selected layers to reduce computation.  
âœ… **Gradient Compression** â†’ Reduces communication overhead during federated training.  
âœ… **Asynchronous & Selective Training** â†’ Optimizes training cycles based on device status.  
âœ… **Secure Updates** â†’ Implements Differential Privacy for data security.  

---
## âš™ï¸ Installation
### **1ï¸âƒ£ Prerequisites**
Ensure you have the following installed:
- Python 3.8+
- PyTorch  
- Flower (FLWR)  
- MLflow for experiment tracking  

### **2ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/ltd-ARYAN-pvt/federated-learning-client.git
cd federated-learning-client
```

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

---
## ğŸš€ Running the Project
### **1ï¸âƒ£ Start the FLWR Server and CLients**
```bash
python train.py --disable-warnings
```


---
## ğŸ”¥ Features & Optimizations
### **1ï¸âƒ£ Model Optimization**  
ğŸ”¹ **Quantization** â†’ Converts 32-bit weights to 8-bit to reduce memory usage.  
ğŸ”¹ **Pruning** â†’ Removes unnecessary neurons to reduce computations.  

### **2ï¸âƒ£ Partial Local Training**  
ğŸ”¹ Only fine-tunes the **last few layers**, reducing RAM & CPU usage.  

### **3ï¸âƒ£ Efficient Data Transmission**  
ğŸ”¹ **Gradient Compression** â†’ Sends only important updates to the FL server.  

### **4ï¸âƒ£ Asynchronous & Selective Training**  
ğŸ”¹ **Trains only when needed** (e.g., when connected to WiFi or power).  

### **5ï¸âƒ£ Secure Client Updates**  
ğŸ”¹ Implements **Differential Privacy (DP)** to protect sensitive data.  

---
## ğŸ“Š MLflow Experiment Tracking
All training metrics (loss, accuracy, memory usage) are logged in **MLflow**.
```bash
mlflow ui
```
Open **`http://localhost:5000`** to visualize training performance.

---
## ğŸ¤ Contributing
Want to improve this project? Feel free to **fork & submit a pull request**!  

---
## ğŸ“œ License
This project is licensed under the **MIT License**.

---
## ğŸ”— References
- [Flower Federated Learning](https://flower.ai/)
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
- [PyTorch Pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)
- [Flower Custom Client](https://flower.ai/docs/framework/tutorial-series-customize-the-client-pytorch.html)
